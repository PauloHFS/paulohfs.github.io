"use strict";(self.webpackChunkpaulohernane_me=self.webpackChunkpaulohernane_me||[]).push([[5447],{9463:(e,i,a)=>{a.r(i),a.d(i,{assets:()=>c,contentTitle:()=>s,default:()=>h,frontMatter:()=>r,metadata:()=>o,toc:()=>l});var n=a(4848),t=a(5680);const r={id:"how-it-works",title:"How it works",tags:["naive bayes","machine learning","data science"]},s=void 0,o={id:"data-science/machine-learning/machine-learning-and-data-science-course/classification/naive-bayes/how-it-works",title:"How it works",description:"1. We create a probability table for each class.",source:"@site/my-brain/data-science/machine-learning/machine-learning-and-data-science-course/classification/naive-bayes/how-it-works.md",sourceDirName:"data-science/machine-learning/machine-learning-and-data-science-course/classification/naive-bayes",slug:"/data-science/machine-learning/machine-learning-and-data-science-course/classification/naive-bayes/how-it-works",permalink:"/my-brain/data-science/machine-learning/machine-learning-and-data-science-course/classification/naive-bayes/how-it-works",draft:!1,unlisted:!1,tags:[{label:"naive bayes",permalink:"/my-brain/tags/naive-bayes"},{label:"machine learning",permalink:"/my-brain/tags/machine-learning"},{label:"data science",permalink:"/my-brain/tags/data-science"}],version:"current",frontMatter:{id:"how-it-works",title:"How it works",tags:["naive bayes","machine learning","data science"]},sidebar:"myBrainSidebar",previous:{title:"naive-bayes",permalink:"/my-brain/data-science/machine-learning/machine-learning-and-data-science-course/classification/naive-bayes/"},next:{title:"svm",permalink:"/my-brain/data-science/machine-learning/machine-learning-and-data-science-course/classification/svm/"}},c={},l=[{value:"Problems",id:"problems",level:2},{value:"apriori and posteriori probabilities",id:"apriori-and-posteriori-probabilities",level:2},{value:"pros",id:"pros",level:2},{value:"cons",id:"cons",level:2}];function b(e){const i={h2:"h2",img:"img",li:"li",ol:"ol",p:"p",ul:"ul",...(0,t.RP)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsxs)(i.ol,{children:["\n",(0,n.jsx)(i.li,{children:"We create a probability table for each class."}),"\n",(0,n.jsx)(i.li,{children:"We calculate the prior probabilities for each class."}),"\n"]}),"\n",(0,n.jsxs)(i.p,{children:["Given this dataset:\n",(0,n.jsx)(i.img,{alt:"alt text",src:a(985).A+"",width:"720",height:"377"}),'\nOur wanted variable is "Risco", so we have to calculate the probabilities for each class.']}),"\n",(0,n.jsxs)(i.p,{children:["The prior probabilities are:\n",(0,n.jsx)(i.img,{alt:"alt text",src:a(2467).A+"",width:"720",height:"351"}),'\nHere we have a example, the probability of "Risco" being "Alto" is 6/14, then goes on.\nThe probability of "Hist\xf3ria de cr\xe9dito" be "Boa" and "Risco" be "Alto" is 1/6.\nThis is done for all the classes.']}),"\n",(0,n.jsxs)(i.p,{children:["If this table, we can estimate the probability of a new instance being in a class. We just need to multiply the probabilities of the features for that class.\n",(0,n.jsx)(i.img,{alt:"alt text",src:a(7976).A+"",width:"720",height:"402"})]}),"\n",(0,n.jsx)(i.p,{children:"The result is the most probable class!"}),"\n",(0,n.jsx)(i.p,{children:"This is the basic idea behind Naive Bayes. It is simple and fast, and it is often used as a baseline for comparison with other more complex algorithms."}),"\n",(0,n.jsx)(i.h2,{id:"problems",children:"Problems"}),"\n",(0,n.jsx)(i.p,{children:"Sometimes the probability can be zero. This can be a problem because it will make the final probability zero. To solve this, we can use Laplace smoothing. This is a technique that adds a small value to the probability to avoid zero probabilities.\nThis is done by adding 1 to the numerator and the number of classes to the denominator. As we are inserting new data into the model, the probability will be adjusted, this can be a problem because the model can be biased."}),"\n",(0,n.jsx)(i.h2,{id:"apriori-and-posteriori-probabilities",children:"apriori and posteriori probabilities"}),"\n",(0,n.jsx)(i.p,{children:"We use apriori probabilities to calculate the probability of a new instance being in a class. This is done by multiplying the probabilities of the features for that class. The result is the posteriori probability."}),"\n",(0,n.jsx)(i.h2,{id:"pros",children:"pros"}),"\n",(0,n.jsxs)(i.ul,{children:["\n",(0,n.jsx)(i.li,{children:"Simple and fast"}),"\n",(0,n.jsx)(i.li,{children:"Good for small datasets"}),"\n"]}),"\n",(0,n.jsx)(i.h2,{id:"cons",children:"cons"}),"\n",(0,n.jsxs)(i.ul,{children:["\n",(0,n.jsx)(i.li,{children:"Assumes that the features are independent (which is not always true) (example: debt and income can be related, but Naive Bayes assumes they are not)"}),"\n"]})]})}function h(e={}){const{wrapper:i}={...(0,t.RP)(),...e.components};return i?(0,n.jsx)(i,{...e,children:(0,n.jsx)(b,{...e})}):b(e)}},5680:(e,i,a)=>{a.d(i,{RP:()=>l});var n=a(6540);function t(e,i,a){return i in e?Object.defineProperty(e,i,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[i]=a,e}function r(e,i){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);i&&(n=n.filter((function(i){return Object.getOwnPropertyDescriptor(e,i).enumerable}))),a.push.apply(a,n)}return a}function s(e){for(var i=1;i<arguments.length;i++){var a=null!=arguments[i]?arguments[i]:{};i%2?r(Object(a),!0).forEach((function(i){t(e,i,a[i])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):r(Object(a)).forEach((function(i){Object.defineProperty(e,i,Object.getOwnPropertyDescriptor(a,i))}))}return e}function o(e,i){if(null==e)return{};var a,n,t=function(e,i){if(null==e)return{};var a,n,t={},r=Object.keys(e);for(n=0;n<r.length;n++)a=r[n],i.indexOf(a)>=0||(t[a]=e[a]);return t}(e,i);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(n=0;n<r.length;n++)a=r[n],i.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(t[a]=e[a])}return t}var c=n.createContext({}),l=function(e){var i=n.useContext(c),a=i;return e&&(a="function"==typeof e?e(i):s(s({},i),e)),a},b={inlineCode:"code",wrapper:function(e){var i=e.children;return n.createElement(n.Fragment,{},i)}},h=n.forwardRef((function(e,i){var a=e.components,t=e.mdxType,r=e.originalType,c=e.parentName,h=o(e,["components","mdxType","originalType","parentName"]),d=l(a),p=t,m=d["".concat(c,".").concat(p)]||d[p]||b[p]||r;return a?n.createElement(m,s(s({ref:i},h),{},{components:a})):n.createElement(m,s({ref:i},h))}));h.displayName="MDXCreateElement"},2467:(e,i,a)=>{a.d(i,{A:()=>n});const n=a.p+"assets/images/image-1-89ea5aa35253a860372c97ee1242596e.png"},7976:(e,i,a)=>{a.d(i,{A:()=>n});const n=a.p+"assets/images/image-2-eb0f0665b45711cf1fbae22d28e39a2e.png"},985:(e,i,a)=>{a.d(i,{A:()=>n});const n=a.p+"assets/images/image-21652d1e7bc773a3c62ecc020f4c4368.png"}}]);