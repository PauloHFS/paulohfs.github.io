"use strict";(self.webpackChunkpaulohernane_me=self.webpackChunkpaulohernane_me||[]).push([[6736],{1394:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>o,contentTitle:()=>c,default:()=>m,frontMatter:()=>s,metadata:()=>i,toc:()=>l});const i=JSON.parse('{"id":"data-science/machine-learning/machine-learning-and-data-science-course/dimensionality-reduction","title":"Dimensionality Reduction","description":"Feature selection X Feature extraction","source":"@site/my-brain/data-science/machine-learning/machine-learning-and-data-science-course/dimensionality-reduction.md","sourceDirName":"data-science/machine-learning/machine-learning-and-data-science-course","slug":"/data-science/machine-learning/machine-learning-and-data-science-course/dimensionality-reduction","permalink":"/my-brain/data-science/machine-learning/machine-learning-and-data-science-course/dimensionality-reduction","draft":false,"unlisted":false,"tags":[{"inline":true,"label":"machine-learning","permalink":"/my-brain/tags/machine-learning"},{"inline":true,"label":"dimensionality-reduction","permalink":"/my-brain/tags/dimensionality-reduction"},{"inline":true,"label":"feature-selection","permalink":"/my-brain/tags/feature-selection"},{"inline":true,"label":"feature-extraction","permalink":"/my-brain/tags/feature-extraction"}],"version":"current","frontMatter":{"id":"dimensionality-reduction","title":"Dimensionality Reduction","tags":["machine-learning","dimensionality-reduction","feature-selection","feature-extraction"]},"sidebar":"myBrainSidebar","previous":{"title":"Variables Types","permalink":"/my-brain/data-science/machine-learning/machine-learning-and-data-science-course/data-pre-processing/variables-types"},"next":{"title":"Feature Engineering and Selection","permalink":"/my-brain/data-science/machine-learning/machine-learning-and-data-science-course/feature-engineering-and-selection"}}');var t=a(4848),r=a(8453);const s={id:"dimensionality-reduction",title:"Dimensionality Reduction",tags:["machine-learning","dimensionality-reduction","feature-selection","feature-extraction"]},c=void 0,o={},l=[{value:"Feature selection X Feature extraction",id:"feature-selection-x-feature-extraction",level:2},{value:"PCA",id:"pca",level:2},{value:"Kernel PCA",id:"kernel-pca",level:2},{value:"LDA",id:"lda",level:2}];function d(e){const n={code:"code",h2:"h2",p:"p",pre:"pre",...(0,r.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h2,{id:"feature-selection-x-feature-extraction",children:"Feature selection X Feature extraction"}),"\n",(0,t.jsx)(n.p,{children:"Feature selection is the process of selecting a subset of the most important features from a dataset."}),"\n",(0,t.jsx)(n.p,{children:"Feature extraction is the process of creating new features from the existing features in a dataset. Dimensionality reduction techniques such as PCA and LDA are examples of feature extraction."}),"\n",(0,t.jsx)(n.p,{children:"Dimensionality Reduction is the process of reducing the number of features in a dataset by selecting a subset of the most important features or by transforming the data into a lower-dimensional space."}),"\n",(0,t.jsx)(n.h2,{id:"pca",children:"PCA"}),"\n",(0,t.jsx)(n.p,{children:"Principal Component Analysis (PCA) is a technique used to reduce the dimensionality of a dataset by transforming the data into a lower-dimensional space while preserving as much variance as possible."}),"\n",(0,t.jsx)(n.p,{children:"PCA works indentifyng the correlation between the features and creating new features that are linear combinations of the original features."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"from sklearn.decomposition import PCA\n\n# Create a PCA object\npca = PCA(n_components=6)\n\n# Fit the object to the data and transform the data\nX_pca = pca.fit_transform(X)\n"})}),"\n",(0,t.jsx)(n.h2,{id:"kernel-pca",children:"Kernel PCA"}),"\n",(0,t.jsx)(n.p,{children:"Kernel PCA is an extension of PCA that uses kernel methods to perform non-linear dimensionality reduction."}),"\n",(0,t.jsx)(n.p,{children:"Kernel PCA works by mapping the data into a higher-dimensional space using a kernel function and then performing PCA in the higher-dimensional space."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"from sklearn.decomposition import KernelPCA\n\n# Create a KernelPCA object\nkpca = KernelPCA(n_components=6, kernel='rbf')\n\n# Fit the object to the data and transform the data\nX_kpca = kpca.fit_transform(X)\n"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.code,{children:"kernel='rbf'"})," is the Radial Basis Function (RBF) kernel, which is commonly used in Kernel PCA. This is a non-linear kernel that can capture complex patterns in the data."]}),"\n",(0,t.jsx)(n.h2,{id:"lda",children:"LDA"}),"\n",(0,t.jsx)(n.p,{children:"Linear Discriminant Analysis (LDA) is a technique used to reduce the dimensionality of a dataset by transforming the data into a lower-dimensional space while maximizing the separation between classes."}),"\n",(0,t.jsx)(n.p,{children:"LDA works by finding the directions (linear discriminants) that maximize the separation between classes."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\n# Create a LDA object\nlda = LinearDiscriminantAnalysis(n_components=6)\n\n# Fit the object to the data and transform the data\nX_lda = lda.fit_transform(X, y)\n"})})]})}function m(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,n,a)=>{a.d(n,{R:()=>s,x:()=>c});var i=a(6540);const t={},r=i.createContext(t);function s(e){const n=i.useContext(r);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function c(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:s(e.components),i.createElement(r.Provider,{value:n},e.children)}}}]);