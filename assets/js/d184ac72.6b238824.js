"use strict";(self.webpackChunkpaulohernane_me=self.webpackChunkpaulohernane_me||[]).push([[6553],{5847:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>o,contentTitle:()=>c,default:()=>h,frontMatter:()=>s,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"data-science/machine-learning/machine-learning-and-data-science-course/feature-engineering-and-selection","title":"Feature Engineering and Selection","description":"It\'s note true that more atributtes will make a model better.","source":"@site/my-brain/data-science/machine-learning/machine-learning-and-data-science-course/feature-engineering-and-selection.md","sourceDirName":"data-science/machine-learning/machine-learning-and-data-science-course","slug":"/data-science/machine-learning/machine-learning-and-data-science-course/feature-engineering-and-selection","permalink":"/my-brain/data-science/machine-learning/machine-learning-and-data-science-course/feature-engineering-and-selection","draft":false,"unlisted":false,"tags":[{"inline":true,"label":"machine-learning","permalink":"/my-brain/tags/machine-learning"},{"inline":true,"label":"feature-engineering","permalink":"/my-brain/tags/feature-engineering"},{"inline":true,"label":"feature-selection","permalink":"/my-brain/tags/feature-selection"},{"inline":true,"label":"data-preprocessing","permalink":"/my-brain/tags/data-preprocessing"}],"version":"current","frontMatter":{"id":"feature-engineering-and-selection","title":"Feature Engineering and Selection","tags":["machine-learning","feature-engineering","feature-selection","data-preprocessing"]},"sidebar":"myBrainSidebar","previous":{"title":"Dimensionality Reduction","permalink":"/my-brain/data-science/machine-learning/machine-learning-and-data-science-course/dimensionality-reduction"},"next":{"title":"Model Validation","permalink":"/my-brain/data-science/machine-learning/machine-learning-and-data-science-course/model-validation"}}');var r=a(4848),i=a(8453);const s={id:"feature-engineering-and-selection",title:"Feature Engineering and Selection",tags:["machine-learning","feature-engineering","feature-selection","data-preprocessing"]},c=void 0,o={},l=[{value:"Low variance",id:"low-variance",level:2},{value:"High correlation",id:"high-correlation",level:2},{value:"Extra tree",id:"extra-tree",level:2}];function d(e){const n={code:"code",h2:"h2",p:"p",pre:"pre",...(0,i.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.p,{children:"It's note true that more atributtes will make a model better.\nSo we need to select the best features to use in our model."}),"\n",(0,r.jsx)(n.h2,{id:"low-variance",children:"Low variance"}),"\n",(0,r.jsx)(n.p,{children:"Variance is a measure of how much the value is spread out (has a large range).\nIf a feature has a low variance, it means that the values are very similar."}),"\n",(0,r.jsxs)(n.p,{children:["We can use the ",(0,r.jsx)(n.code,{children:"VarianceThreshold"})," class from ",(0,r.jsx)(n.code,{children:"sklearn.feature_selection"})," to remove features with low variance."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"from sklearn.feature_selection import VarianceThreshold\n\n# Create a VarianceThreshold object\nselector = VarianceThreshold(threshold=0.2)\n\n# Fit the object to the data\nselector.fit(data)\n\n# Get the indices of the features that are being kept\nkept_indices = selector.get_support(indices=True)\n\n# Get the names of the features that are being kept\nkept_features = data.columns[kept_indices]\n\n# Transform the data to only keep the selected features\ndata_selected = selector.transform(data)\n"})}),"\n",(0,r.jsxs)(n.p,{children:["To get the threshold value, we can use the ",(0,r.jsx)(n.code,{children:"np.var"})," function from NumPy to calculate the variance of each feature."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# Calculate the variance of each feature\nvariances = np.var(data, axis=0) # axis=0 calculates the variance of each column\n\n# Get the names of the features with low variance\nlow_variance_features = data.columns[variances < 0.2]\n"})}),"\n",(0,r.jsx)(n.h2,{id:"high-correlation",children:"High correlation"}),"\n",(0,r.jsx)(n.p,{children:"Correlation is a measure of how much two variables change together.\nIf two features are highly correlated, they may contain redundant information."}),"\n",(0,r.jsxs)(n.p,{children:["We can use the ",(0,r.jsx)(n.code,{children:"corr"})," method from a pandas DataFrame to calculate the correlation between features."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# Calculate the correlation matrix\ncorrelation_matrix = data.corr()\n\n# Get the indices of the features that are highly correlated\nhighly_correlated_indices = np.where(np.abs(correlation_matrix) > 0.8)\n\n# Get the names of the features that are highly correlated\nhighly_correlated_features = data.columns[highly_correlated_indices]\n"})}),"\n",(0,r.jsx)(n.h2,{id:"extra-tree",children:"Extra tree"}),"\n",(0,r.jsx)(n.p,{children:"Extra Trees is an ensemble learning method that fits multiple decision trees to the data and averages the predictions.\nThis uses the Extra Trees not for classification or regression, but to calculate the feature importance."}),"\n",(0,r.jsxs)(n.p,{children:["We can use the ",(0,r.jsx)(n.code,{children:"ExtraTreesClassifier"})," and ",(0,r.jsx)(n.code,{children:"ExtraTreesRegressor"})," classes from ",(0,r.jsx)(n.code,{children:"sklearn.ensemble"})," to calculate the feature importance."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"from sklearn.ensemble import ExtraTreesClassifier\n\n# Split the data into training and testing sets\ndata = df.drop('target', axis=1) # axis=1 drops the 'target' column\ntarget = df['target']\n\n# Create an ExtraTreesClassifier object\nmodel = ExtraTreesClassifier(criterion='gini', random_state=42)\n\n# Fit the model to the data\nmodel.fit(data, target)\n\n# Get the feature importances\nfeature_importances = model.feature_importances_\n\n# Drop the features with low importance\ndata_selected = data.drop(data.columns[feature_importances < 0.1], axis=1)\n\n# Get the names of the features with low importance\nlow_importance_features = data.columns[feature_importances < 0.1]\n"})})]})}function h(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},8453:(e,n,a)=>{a.d(n,{R:()=>s,x:()=>c});var t=a(6540);const r={},i=t.createContext(r);function s(e){const n=t.useContext(i);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function c(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:s(e.components),t.createElement(i.Provider,{value:n},e.children)}}}]);