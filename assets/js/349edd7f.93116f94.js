"use strict";(self.webpackChunkpaulohernane_me=self.webpackChunkpaulohernane_me||[]).push([[5447],{1921:(e,i,a)=>{a.d(i,{A:()=>n});const n=a.p+"assets/images/image-1-89ea5aa35253a860372c97ee1242596e.png"},2487:(e,i,a)=>{a.d(i,{A:()=>n});const n=a.p+"assets/images/image-21652d1e7bc773a3c62ecc020f4c4368.png"},4490:(e,i,a)=>{a.d(i,{A:()=>n});const n=a.p+"assets/images/image-2-eb0f0665b45711cf1fbae22d28e39a2e.png"},8276:(e,i,a)=>{a.r(i),a.d(i,{assets:()=>l,contentTitle:()=>o,default:()=>d,frontMatter:()=>r,metadata:()=>n,toc:()=>c});const n=JSON.parse('{"id":"data-science/machine-learning/machine-learning-and-data-science-course/classification/naive-bayes/how-it-works","title":"How it works","description":"1. We create a probability table for each class.","source":"@site/my-brain/data-science/machine-learning/machine-learning-and-data-science-course/classification/naive-bayes/how-it-works.md","sourceDirName":"data-science/machine-learning/machine-learning-and-data-science-course/classification/naive-bayes","slug":"/data-science/machine-learning/machine-learning-and-data-science-course/classification/naive-bayes/how-it-works","permalink":"/my-brain/data-science/machine-learning/machine-learning-and-data-science-course/classification/naive-bayes/how-it-works","draft":false,"unlisted":false,"tags":[{"inline":true,"label":"naive bayes","permalink":"/my-brain/tags/naive-bayes"},{"inline":true,"label":"machine learning","permalink":"/my-brain/tags/machine-learning"},{"inline":true,"label":"data science","permalink":"/my-brain/tags/data-science"}],"version":"current","frontMatter":{"id":"how-it-works","title":"How it works","tags":["naive bayes","machine learning","data science"]},"sidebar":"myBrainSidebar","previous":{"title":"naive-bayes","permalink":"/my-brain/data-science/machine-learning/machine-learning-and-data-science-course/classification/naive-bayes/"},"next":{"title":"svm","permalink":"/my-brain/data-science/machine-learning/machine-learning-and-data-science-course/classification/svm/"}}');var s=a(4848),t=a(8453);const r={id:"how-it-works",title:"How it works",tags:["naive bayes","machine learning","data science"]},o=void 0,l={},c=[{value:"Problems",id:"problems",level:2},{value:"apriori and posteriori probabilities",id:"apriori-and-posteriori-probabilities",level:2},{value:"pros",id:"pros",level:2},{value:"cons",id:"cons",level:2}];function h(e){const i={h2:"h2",img:"img",li:"li",ol:"ol",p:"p",ul:"ul",...(0,t.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsxs)(i.ol,{children:["\n",(0,s.jsx)(i.li,{children:"We create a probability table for each class."}),"\n",(0,s.jsx)(i.li,{children:"We calculate the prior probabilities for each class."}),"\n"]}),"\n",(0,s.jsxs)(i.p,{children:["Given this dataset:\n",(0,s.jsx)(i.img,{alt:"alt text",src:a(2487).A+"",width:"720",height:"377"}),'\nOur wanted variable is "Risco", so we have to calculate the probabilities for each class.']}),"\n",(0,s.jsxs)(i.p,{children:["The prior probabilities are:\n",(0,s.jsx)(i.img,{alt:"alt text",src:a(1921).A+"",width:"720",height:"351"}),'\nHere we have a example, the probability of "Risco" being "Alto" is 6/14, then goes on.\nThe probability of "Hist\xf3ria de cr\xe9dito" be "Boa" and "Risco" be "Alto" is 1/6.\nThis is done for all the classes.']}),"\n",(0,s.jsxs)(i.p,{children:["If this table, we can estimate the probability of a new instance being in a class. We just need to multiply the probabilities of the features for that class.\n",(0,s.jsx)(i.img,{alt:"alt text",src:a(4490).A+"",width:"720",height:"402"})]}),"\n",(0,s.jsx)(i.p,{children:"The result is the most probable class!"}),"\n",(0,s.jsx)(i.p,{children:"This is the basic idea behind Naive Bayes. It is simple and fast, and it is often used as a baseline for comparison with other more complex algorithms."}),"\n",(0,s.jsx)(i.h2,{id:"problems",children:"Problems"}),"\n",(0,s.jsx)(i.p,{children:"Sometimes the probability can be zero. This can be a problem because it will make the final probability zero. To solve this, we can use Laplace smoothing. This is a technique that adds a small value to the probability to avoid zero probabilities.\nThis is done by adding 1 to the numerator and the number of classes to the denominator. As we are inserting new data into the model, the probability will be adjusted, this can be a problem because the model can be biased."}),"\n",(0,s.jsx)(i.h2,{id:"apriori-and-posteriori-probabilities",children:"apriori and posteriori probabilities"}),"\n",(0,s.jsx)(i.p,{children:"We use apriori probabilities to calculate the probability of a new instance being in a class. This is done by multiplying the probabilities of the features for that class. The result is the posteriori probability."}),"\n",(0,s.jsx)(i.h2,{id:"pros",children:"pros"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsx)(i.li,{children:"Simple and fast"}),"\n",(0,s.jsx)(i.li,{children:"Good for small datasets"}),"\n"]}),"\n",(0,s.jsx)(i.h2,{id:"cons",children:"cons"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsx)(i.li,{children:"Assumes that the features are independent (which is not always true) (example: debt and income can be related, but Naive Bayes assumes they are not)"}),"\n"]})]})}function d(e={}){const{wrapper:i}={...(0,t.R)(),...e.components};return i?(0,s.jsx)(i,{...e,children:(0,s.jsx)(h,{...e})}):h(e)}},8453:(e,i,a)=>{a.d(i,{R:()=>r,x:()=>o});var n=a(6540);const s={},t=n.createContext(s);function r(e){const i=n.useContext(t);return n.useMemo((function(){return"function"==typeof e?e(i):{...i,...e}}),[i,e])}function o(e){let i;return i=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),n.createElement(t.Provider,{value:i},e.children)}}}]);