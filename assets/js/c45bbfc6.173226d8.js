"use strict";(self.webpackChunkpaulohernane_me=self.webpackChunkpaulohernane_me||[]).push([[6360],{6517:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>o,contentTitle:()=>s,default:()=>m,frontMatter:()=>i,metadata:()=>c,toc:()=>d});var t=a(4848),r=a(5680);const i={id:"data-standardization",title:"Data Standardization"},s=void 0,c={id:"data-science/machine-learning/machine-learning-and-data-science-course/data-pre-processing/data-standardization",title:"Data Standardization",description:"Whem working with some algorithms, we need to standardize the data to a common scale, cause some algorithms are sensitive to the scale of the input data.",source:"@site/my-brain/data-science/machine-learning/machine-learning-and-data-science-course/data-pre-processing/data-standardization.md",sourceDirName:"data-science/machine-learning/machine-learning-and-data-science-course/data-pre-processing",slug:"/data-science/machine-learning/machine-learning-and-data-science-course/data-pre-processing/data-standardization",permalink:"/my-brain/data-science/machine-learning/machine-learning-and-data-science-course/data-pre-processing/data-standardization",draft:!1,unlisted:!1,tags:[],version:"current",frontMatter:{id:"data-standardization",title:"Data Standardization"},sidebar:"myBrainSidebar",previous:{title:"Categorical Variables",permalink:"/my-brain/data-science/machine-learning/machine-learning-and-data-science-course/data-pre-processing/categorical-variables"},next:{title:"Handle Inconsistent Data",permalink:"/my-brain/data-science/machine-learning/machine-learning-and-data-science-course/data-pre-processing/handle-inconsistent-data"}},o={},d=[{value:"Standardization",id:"standardization",level:2},{value:"Normalization",id:"normalization",level:3}];function l(e){const n={code:"code",h2:"h2",h3:"h3",li:"li",p:"p",pre:"pre",ul:"ul",...(0,r.RP)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.p,{children:"Whem working with some algorithms, we need to standardize the data to a common scale, cause some algorithms are sensitive to the scale of the input data."}),"\n",(0,t.jsx)(n.h2,{id:"standardization",children:"Standardization"}),"\n",(0,t.jsx)(n.p,{children:"x = (x - mean(x)) / standard deviation(x)"}),"\n",(0,t.jsx)(n.p,{children:"Where:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"x is the original value"}),"\n",(0,t.jsx)(n.li,{children:"mean(x) is the mean of the feature x"}),"\n",(0,t.jsx)(n.li,{children:"standard deviation(x) is the standard deviation of the feature x"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"This is more robust to outliers than normalization."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\n# Fit the scaler to the data\nscaler.fit(X_train)\n\n# Transform the training and testing sets\nX_train_scaled = scaler.transform(X_train)\n\nX_test_scaled = scaler.transform(X_test)\n\n# or in one step\nX_train_scaled = scaler.fit_transform(X_train)\n"})}),"\n",(0,t.jsx)(n.h3,{id:"normalization",children:"Normalization"}),"\n",(0,t.jsx)(n.p,{children:"Normalization is another technique used to scale the data to a common scale. It scales the data to a fixed range, usually between 0 and 1."}),"\n",(0,t.jsx)(n.p,{children:"x = (x - min(x)) / (max(x) - min(x))"}),"\n",(0,t.jsx)(n.p,{children:"Where:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"x is the original value"}),"\n",(0,t.jsx)(n.li,{children:"min(x) is the minimum value of the feature x"}),"\n",(0,t.jsx)(n.li,{children:"max(x) is the maximum value of the feature x"}),"\n"]})]})}function m(e={}){const{wrapper:n}={...(0,r.RP)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(l,{...e})}):l(e)}},5680:(e,n,a)=>{a.d(n,{RP:()=>d});var t=a(6540);function r(e,n,a){return n in e?Object.defineProperty(e,n,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[n]=a,e}function i(e,n){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var t=Object.getOwnPropertySymbols(e);n&&(t=t.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),a.push.apply(a,t)}return a}function s(e){for(var n=1;n<arguments.length;n++){var a=null!=arguments[n]?arguments[n]:{};n%2?i(Object(a),!0).forEach((function(n){r(e,n,a[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):i(Object(a)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(a,n))}))}return e}function c(e,n){if(null==e)return{};var a,t,r=function(e,n){if(null==e)return{};var a,t,r={},i=Object.keys(e);for(t=0;t<i.length;t++)a=i[t],n.indexOf(a)>=0||(r[a]=e[a]);return r}(e,n);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(t=0;t<i.length;t++)a=i[t],n.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var o=t.createContext({}),d=function(e){var n=t.useContext(o),a=n;return e&&(a="function"==typeof e?e(n):s(s({},n),e)),a},l={inlineCode:"code",wrapper:function(e){var n=e.children;return t.createElement(t.Fragment,{},n)}},m=t.forwardRef((function(e,n){var a=e.components,r=e.mdxType,i=e.originalType,o=e.parentName,m=c(e,["components","mdxType","originalType","parentName"]),h=d(a),u=r,p=h["".concat(o,".").concat(u)]||h[u]||l[u]||i;return a?t.createElement(p,s(s({ref:n},m),{},{components:a})):t.createElement(p,s({ref:n},m))}));m.displayName="MDXCreateElement"}}]);